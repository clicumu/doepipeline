#!/usr/bin/env python
"""
Optimize a data-processing pipeline specified in config-file using
principles from statistical Design of Experiments (DoE).
"""
import argparse
import os
import sys
import logging

from doepipeline.executor import LocalPipelineExecutor, SlurmPipelineExecutor
from doepipeline.generator import PipelineGenerator


class Range:
    """
    Type checker-class for float input required to be within an interval.
    """
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __eq__(self, other):
        return self.start <= other <= self.end

    def __repr__(self):
        return '{0}-{1}'.format(self.start, self.end)


def reduction_argument(value):
    if value == 'auto':
        return value

    err_msg = '--reduction must be "auto" or positive integer'
    try:
        value = int(value)
    except TypeError:
        raise argparse.ArgumentTypeError(err_msg)
    if value < 1:
        raise argparse.ArgumentTypeError(err_msg)
    return value


def make_parser():
    """ Create and config argument-parser.

    :return: prepared parser.
    :rtype: argparse.ArgumentParser
    """
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('config', help='pipeline config-file')
    parser.add_argument('-e', '--execution', default='serial',
                          help=('how to execute steps in pipeline '
                                '(default serial)'),
                          choices=('serial', 'slurm', 'parallel'))
    parser.add_argument('-w', '--workdir',
                        help='working directory (default: .)',
                        default='.')

    parser.add_argument('-i', '--maxiter', default=10, type=int,
                        choices=[Range(1, 100)],
                        help=('maximum number of iterations (default: 10)'))

    parser.add_argument('--tol', type=float, default=.25, choices=[Range(0, 1)],
                        help=('maximum allowed relative distance between '
                              'predicted optimum to edge of experimental '
                              'design in order to consider optimization '
                              'converged (default: 0.25)'))
    parser.add_argument('--skip_screening', action='store_true',
                        help=('If this flag is set, not screening will be run '
                              'spanning all values from factor mins to factor '
                              'maxes.'))
    parser.add_argument('--screening_reduction', default='auto',
                        type=reduction_argument,
                        help='Reduction factor for GSD-screening (default auto)')

    parser.add_argument('--degree', type=int, choices=[Range(1, 3)],
                        default=2, help='degree of polynomial fitted during '
                                        'optimization (default: 2)')
    parser.add_argument('-o', '--output', default='optimal_parameters.csv',
                        help='output file (default: optimal_parameters.csv)')
    parser.add_argument('-l', '--log', default=None,
                        help='log file (default: STDOUT)')
    parser.add_argument('-d', '--debug', action='store_true',
                        help='if set, logging will be set to debug')

    return parser


if __name__ == '__main__':
    parser = make_parser()
    args = parser.parse_args()

    if args.debug:
        log_format = ('[%(filename)s:%(lineno)s - %(funcName)20s() ] '
                      '%(asctime)s %(message)s')
    else:
        log_format = '%(asctime)s %(levelname)-8s: %(message)s'
    logging.basicConfig(
        format=log_format, filename=args.log,
        level=logging.INFO if not args.debug else logging.DEBUG
    )

    try:
        logging.info('Reads config: {}'.format(args.config))
        generator = PipelineGenerator.from_yaml(args.config)
    except ValueError as e:
        logging.critical('Failed to read config.')
        sys.exit(str(e))

    logging.info('Initialize designer.')
    designer = generator.new_designer_from_config(
        skip_screening=args.skip_screening, gsd_reduction=args.screening_reduction)

    if args.execution == 'slurm':
        executor_class = SlurmPipelineExecutor
    elif args.execution == 'serial':
        executor_class = LocalPipelineExecutor
    elif args.execution == 'parallel':
        executor_class = lambda *a, **kw: LocalPipelineExecutor(*a,
                                                                run_serial=False,
                                                                **kw)
    else:
        sys.exit('Unknown executor: {}'.format(args.execution))

    n_iter = 0
    while n_iter < args.maxiter:
        n_iter += 1

        logging.info('Starts optimization iteration {}.'.format(n_iter))

        iter_workdir = os.path.join(args.workdir, 'iter_'.format(n_iter))
        logging.info('Current working-directory: {}'.format(iter_workdir))

        executor = executor_class(
            workdir=iter_workdir,
            base_command='{script}')

        logging.info('Sets up new design.')
        design = designer.new_design()

        logging.info('Prepares pipeline with design parameters.')
        pipeline = generator.new_pipeline_collection(design)

        logging.info('Start execution of pipeline.')
        results = executor.run_pipeline_collection(pipeline)

        logging.info('Execution iteration {} finished, finding optimum'.format(n_iter))
        optimum = designer.update_factors_from_response(results, tol=args.tol)

        if optimum.converged:
            logging.info(('Optimum reached after {} iterations. '
                          'Saves output to {}.'.format(n_iter, args.output)))
            optimum.predicted_optimum.to_csv(args.output)
            break

    if not optimum.converged:
        logging.info('Failed to converge to optimum in {} iterations'.format(n_iter))
        try:
            prefix, suffix = args.output.rsplit('.', 1)
        except ValueError:
            prefix = args.output
            suffix = ''
        outpath = '.'.join([prefix, 'unconverged', suffix])

        logging.info('Saves unconverged results to {}.'.format(outpath))
        optimum.predicted_optimum.to_csv(outpath)

    logging.info('Shuts down.')
