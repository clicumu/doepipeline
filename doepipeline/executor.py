"""
This module contains executor-classes for pipelines generated by
a instance of :class:`doepipeline.generator.PipelineGenerator`.

Classes:
* :class:`BasePipelineExecutor` - Base-class for all executors. Implements
  basic interface and method to run pipeline collections.

Exceptions:
* :class:`CommandError`
* :class:`PipelineRunFailed`
"""
import abc
import logging
import time
import subprocess
from contextlib import contextmanager

from doepipeline import utils

log = logging.getLogger(__name__)


class CommandError(Exception):
    """
    Raised whenever a command fails.
    """


class PipelineRunFailed(Exception):
    """
    Raised when running pipeline failed.
    """


class BasePipelineExecutor:
    """ Base-class for pipeline executor classes.

    Implements parallel pipeline execution given pipeline collection
    from :class:`doepipeline.generator.PipelineGenerator`. Pipeline
    can either be run in parallel using Linux screens or batch-sripts.

    Attributes:
    :ivar workdir:
    :ivar base_script:
    :ivar base_log:
    :ivar run_in_batch:
    :ivar poll_interval:
    :ivar running_jobs:

    Class attributes:
    :cvar JOB_FINISHED:
    :cvar JOB_RUNNING:
    :cvar JOB_FAILED:
    """

    __metaclass__ = abc.ABCMeta

    JOB_FINISHED = 'job_finished'
    JOB_RUNNING = 'job_running'
    JOB_FAILED = 'job_failed'

    def __init__(self, workdir=None, run_in_batch=False,
                 poll_interval=10, base_command=None, base_log=None):
        assert workdir is None or isinstance(workdir, str) and workdir.strip(),\
            'path must be None or string'
        assert isinstance(run_in_batch, bool), 'run_in_batch must be boolean'
        assert not isinstance(poll_interval, bool) and\
            isinstance(poll_interval, int) and poll_interval > 0,\
            'poll_interval must be positive integer'

        if base_command is not None:
            try:
                self.base_command = utils.validate_command(base_command)
            except AssertionError, e:
                raise ValueError('invalid base-command: ' + e.message)
        else:
             self.base_command = 'nohup {script} > {logfile} 2>&1'

        if base_log is not None:
            try:
                self.base_log = utils.validate_log_file(base_log)
            except AssertionError, e:
                raise ValueError('invalid base-log: ' + e.message)
        else:
            self.base_log = '{name}_step_{i}.log'

        self.workdir = workdir if workdir is not None else '.'
        self.run_in_batch = run_in_batch
        self.poll_interval = poll_interval
        self.running_jobs = dict()

    @abc.abstractmethod
    def execute_command(self, command, watch=False, **kwargs):
        """ Extend to execute the given command in current execution
        environment.

        Base-class provides basic input validation and if `watch` is
        True, the current command is added the instances `current_jobs`.

        :param str command: Command to execute.
        :param bool watch: If True, add command to process to watch.
        :param kwargs: Additional keyword arguments.
        """
        try:
            assert isinstance(command, str) and command.strip(),\
                'command must be a string'
            assert isinstance(watch, bool), 'watch must be boolean'
            if watch:
                assert 'job_name' in kwargs,\
                    'if watch is True, job_name must be given'
        except AssertionError, e:
            raise ValueError(e.message)

        if watch:
            self.running_jobs[kwargs.pop('job_name')] = command

    @abc.abstractmethod
    def poll_jobs(self):
        """ Abstract method.

        Implement method to check status of running jobs.
        The method should return a tuple of two consisting
        of job-status and a message.

        Allowed statuses are:

        * `JOB_RUNNING` / "job_running"
        * `JOB_FINISHED` / "job_finished"
        * `JOB_FAILED` / "job_failed"

        :returns: status, message
        :rtype: str, str
        """

    def run_pipeline_collection(self, pipeline_collection):
        """

        :param pipeline_collection:
        :return:
        """
        # Move to working-directory
        try:
            self._cd(self.workdir)
        except CommandError:
            log.info('{} not found, creating directory'.format(self.workdir))
            self._mkdir(self.workdir)
            self._cd(self.workdir)

        # Initialization..
        pipeline_length = len(next(iter(pipeline_collection.values())))
        experiment_index = list()
        job_steps = [[] for _ in range(pipeline_length)]
        env_variables = pipeline_collection.get('ENV_VARIABLES', None)
        reserved = ['ENV_VARIABLES']

        log.info('Creating job directories.')
        for job_name, scripts in pipeline_collection.items():
            if job_name in reserved:
                # Don't treat special names as jobs.
                continue

            experiment_index.append(job_name)
            log.debug('Creating directory: {}'.format(job_name))
            self._mkdir(job_name)

            for i, script in enumerate(scripts):
                job_steps[i].append(script)

        if self.run_in_batch:
            log.info('Run batch jobs')
            self.run_batches(job_steps, experiment_index, env_variables)
        else:
            log.info('Run jobs in parallel using screens')
            self.run_in_screens(job_steps, experiment_index, env_variables)

    def run_batches(self, job_steps, experiment_index, env_variables):
        """ Run the collection of jobs in parallel using batch execution.

        Example job_steps:
        >>> job_steps = [
        ...     ['./script_1 --opt1 factor1_1', './script_1 --opt1 factor1_2'],
        ...     ['./script_2 --opt2 factor2_1', './script_2 --opt2 factor2_2']
        ... ]

        Given above example, the following batch-scripts will be run::

            cd [exp_name1] && nohup ./script_1 --opt1 factor1_1 > [exp_name1]_step_1.log 2>&1 && cd .. &
            cd [exp_name2] && nohup ./script_1 --opt1 factor1_2 > [exp_name2]_step_1.log 2>&1 && cd .. &
            wait

        And when finished::

            cd [exp_name1] && nohup ./script_2 --opt2 factor2_1 > [exp_name1]_step_2.log 2>&1 && cd .. &
            cd [exp_name2] && nohup ./script_2 --opt2 factor2_2 > [exp_name2]_step_2.log 2>&1 && cd .. &
            wait

        :param job_steps: List of step-wise scripts.
        :type job_steps: list[list]
        :param experiment_index: List of job-names.
        :type experiment_index: list[str]
        :param env_variables: Environment variables to set.
        :type env_variables: dict
        """
        if self.base_command.endswith(' &'):
            base = self.base_command[:-2]
        else:
            base = self.base_command

        if env_variables is not None:
            self._set_env_variables(env_variables)

        base_command= 'cd {job_dir} && {{log_script}}' + base + ' && cd .. &'

        for i, step in enumerate(job_steps, start=1):
            commands = list()
            for script, job_name in zip(step, experiment_index):
                # Prepare log and command.
                log_file = self.base_log.format(name=job_name, i=i)

                try:
                    command = base_command.format(job_dir=job_name,
                                                  script=script)
                except KeyError:
                    command = base_command.format(job_dir=job_name,
                                                  script=script,
                                                  logfile=log_file)
                    log_script = 'touch {logfile} && '.format(logfile=log_file)
                    command = command.format(log_script=log_script)
                else:
                    command = command.format(log_script='')

                commands.append(command)

            commands.append('wait')
            batch_command = ' '.join(commands)
            self.execute_command(batch_command, watch=True,
                                 job_name='batch_{}'.format(i))

            try:
                self._wait_until_current_jobs_are_finished()
            except PipelineRunFailed:
                raise

    def run_in_screens(self, job_steps, experiment_index, env_variables):
        """ Run the collection of jobs in parallel using screens.

        Example job_steps:
        >>> job_steps = [
        ...     ['./script_1 --opt1 factor1_1', './script_1 --opt1 factor1_2'],
        ...     ['./script_2 --opt2 factor2_1', './script_2 --opt2 factor2_2']
        ... ]

        :param job_steps: List of step-wise scripts.
        :type job_steps: list[list]
        :param experiment_index: List of job-names.
        :type experiment_index: list[str]
        """
        base_command = self.base_command
        if not self.base_command.endswith('&'):
            base_command += ' &'

        for job_name in experiment_index:
            log.debug('Setting up screen: {}'.format(job_name))
            self._make_screen(job_name)
            with self.screen(job_name):
                self._cd(job_name)
                if env_variables is not None:
                    self._set_env_variables(env_variables)

        # Run all scripts of each step in parallel using screens.
        # If a step fails, raise PipelineRunFailed.
        for i, step in enumerate(job_steps, start=1):
            for script, job_name in zip(step, experiment_index):
                # Prepare log and command.
                log_file = self.base_log.format(name=job_name, i=i)

                try:
                    command = base_command.format(script=script)
                except KeyError:
                    has_log = True
                    command = base_command.format(script=script,
                                                  logfile=log_file)
                else:
                    has_log = False

                # Execute script in screen.
                with self.screen(job_name):
                    if has_log:
                        self.execute_command('touch {log}'.format(log=log_file))
                    log.debug('Executes: {}'.format(command))
                    self.execute_command(command, watch=True, job_name=job_name)

            try:
                self._wait_until_current_jobs_are_finished()
            except PipelineRunFailed:
                raise

    @contextmanager
    def screen(self, screen_name):
        """ Context-manager to run commands within Linux-screens.

        :param screen_name: Name of screen to connect to.
        """
        self._reconnect_screen(screen_name)
        try:
            yield
        finally:
            self._disconnect_current_screen()

    def _wait_until_current_jobs_are_finished(self):
        # Monitor job status.
        while 'running':
            status, msg = self.poll_jobs()
            if status == BasePipelineExecutor.JOB_FINISHED:
                self.running_jobs = dict()
                break
            elif status == BasePipelineExecutor.JOB_RUNNING:
                time.sleep(self.poll_interval)
            else:
                self.running_jobs = dict()
                raise PipelineRunFailed(msg)

    def _disconnect_current_screen(self):
        self.execute_command('screen -d')

    def _reconnect_screen(self, name):
        self.execute_command('screen -r {}'.format(name))

    def _make_screen(self, name):
        self.execute_command('screen -S {}'.format(name))
        self._disconnect_current_screen()

    def _cd(self, dir):
        self.execute_command('cd {}'.format(dir))

    def _mkdir(self, dir):
        self.execute_command('mkdir {}'.format(dir))

    def _set_env_variables(self, env_variables):
        for name, value in env_variables.items():
            self.execute_command('{}={}'.format(name, value))


class LocalPipelineExecutor(BasePipelineExecutor):

    """
    Executor class running pipeline locally.
    """
    def __init__(self, *args, **kwargs):
        super(LocalPipelineExecutor, self).__init__(*args, **kwargs)
        self.running_jobs = dict()

    def poll_jobs(self):
        still_running = list()
        for job_name, process in self.running_jobs.items():
            if process.poll() is None:
                still_running.append(job_name)
            else:
                if process.return_code != 0:
                    return self.JOB_FAILED, '{} has failed'.format(job_name)
                else:
                    self.running_jobs.pop(job_name)

        if still_running:
            msg = '{} still running'.format(', '.join(still_running))
            return self.JOB_RUNNING, msg
        else:
            return self.JOB_FINISHED, 'no jobs running.'

    def execute_command(self, command, watch=False, **kwargs):
        """ Execute given command by executing it in subprocess.

        Calls are made using `subprocess`-module like::

            process = subprocess.Popen(command, shell=True)

        :param str command: Command to execute.
        :param bool watch: If True, monitor process.
        :param kwargs: Keyword-arguments.
        """
        super(LocalPipelineExecutor, self).execute_command(command, watch,
                                                           **kwargs)
        process = subprocess.Popen(command, shell=True)
        if watch:
            self.running_jobs[kwargs.pop('job_name')] = process