"""
This module contains executor-classes for pipelines generated by
a instance of :class:`doepipeline.generator.PipelineGenerator`.

Classes:
* :class:`BasePipelineExecutor` - Base-class for all executors. Implements
  basic interface and method to run pipeline collections.

Exceptions:
* :class:`CommandError`
* :class:`PipelineRunFailed`
"""
import abc
import logging
import time
from contextlib import contextmanager

log = logging.getLogger(__name__)


class CommandError(Exception):
    """
    Raised whenever a command fails.
    """


class PipelineRunFailed(Exception):
    """
    Raised when running pipeline failed.
    """


class BasePipelineExecutor:

    __metaclass__ = abc.ABCMeta

    JOB_FINISHED = 'job_finished'
    JOB_RUNNING = 'job_running'
    JOB_FAILED = 'job_failed'

    def __init__(self, workdir=None, max_jobs=None, run_in_batch=False,
                 poll_interval=10):
        self.workdir = workdir if workdir is not None else '.'
        self.max_jobs = max_jobs if max_jobs is not None else float('inf')
        self.run_in_batch = run_in_batch
        self.poll_interval = poll_interval
        self.running_jobs = dict()

    @abc.abstractmethod
    def connect(self, *args, **kwargs):
        pass

    @abc.abstractmethod
    def disconnect(self, *args, **kwargs):
        pass

    @abc.abstractmethod
    def execute_command(self, *args, **kwargs):
        pass

    @abc.abstractmethod
    def poll_jobs(self):
        pass

    def run_pipeline_collection(self, pipeline_collection):
        """

        :param pipeline_collection:
        :return:
        """
        # Move to working-directory
        try:
            self._cd(self.workdir)
        except CommandError:
            log.info('{} not found, creating directory'.format(self.workdir))
            self._mkdir(self.workdir)
            self._cd(self.workdir)

        # Initialization..
        pipeline_length = len(next(iter(pipeline_collection.values())))
        experiment_index = list()
        job_steps = [[] for _ in range(pipeline_length)]

        log.info('Creating job directories.')
        for job_name, scripts in pipeline_collection.items():
            experiment_index.append(job_name)
            log.debug('Creating directory: {}'.format(job_name))
            self._mkdir(job_name)

            for i, script in enumerate(scripts):
                job_steps[i].append(script)

        if self.run_in_batch:
            log.info('Run batch jobs')
            self.run_batches(job_steps, experiment_index)
        else:
            log.info('Run jobs in parallel using screens')
            self.run_in_screens(job_steps, experiment_index)

    def run_in_screens(self, job_steps, experiment_index):
        """ Run the collection of jobs in parallel using screens.

        Example job_steps:
        >>> job_steps = [
        ...     ['./script_1 --opt1 factor1_1', './script_1 --opt1 factor1_2'],
        ...     ['./script_2 --opt2 factor2_1', './script_2 --opt2 factor2_2']
        ... ]

        :param job_steps: List of step-wise scripts.
        :type job_steps: list[list]
        :param experiment_index: List of job-names.
        :type experiment_index: list[str]
        """
        base_command = 'nohup {script} > {logfile} 2>&1 &\n echo $!'

        for job_name in experiment_index:
            log.debug('Setting up screen: {}'.format(job_name))
            self._make_screen(job_name)
            with self.screen(job_name):
                self._cd(job_name)

        # Run all scripts of each step in parallel using screens.
        # If a step fails, raise PipelineRunFailed.
        for i, step in enumerate(job_steps, start=1):
            for script, job_name in zip(step, experiment_index):
                # Prepare log and command.
                log_file = '{name}_step_{i}.log'.format(name=job_name, i=i)
                command = base_command.format(script=script, logfile=log_file)

                # Execute script in screen.
                with self.screen(job_name):
                    self.execute_command('touch {log}'.format(log=log_file))
                    log.debug('Executes: {}'.format(command))
                    self.execute_command(command)
                    self.running_jobs[job_name] = command

            # Monitor job status.
            while 'running':
                status, msg = self.poll_jobs()
                if status == BasePipelineExecutor.JOB_FINISHED:
                    self.running_jobs = dict()
                    break
                elif status == BasePipelineExecutor.JOB_RUNNING:
                    time.sleep(self.poll_interval)
                else:
                    self.running_jobs = dict()
                    raise PipelineRunFailed(msg)

    @contextmanager
    def screen(self, screen_name):
        """ Context-manager to run commands within Linux-screens.

        :param screen_name: Name of screen to connect to.
        """
        self._reconnect_screen(screen_name)
        try:
            yield
        finally:
            self._disconnect_current_screen()

    def _disconnect_current_screen(self):
        self.execute_command('screen -d')

    def _reconnect_screen(self, name):
        self.execute_command('screen -r {}'.format(name))

    def _make_screen(self, name):
        self.execute_command('screen -S {}'.format(name))
        self._disconnect_current_screen()

    def _cd(self, dir):
        self.execute_command('cd {}'.format(dir))

    def _mkdir(self, dir):
        self.execute_command('mkdir {}'.format(dir))

